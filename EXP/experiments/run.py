import sys
import os
import yaml
import torch
import numpy as np
import argparse
from datetime import datetime

# Add path to sys.path to find proto/diag modules
base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(base_path)
print(f"DEBUG: sys.path augmented with: {base_path}")
print(f"DEBUG: Available dirs in base_path: {os.listdir(base_path) if os.path.exists(base_path) else 'Path not found'}")

from proto.env.point_mass import PointMassEnv
from proto.kernel.kernels import SymplecticKernel, ContactKernel, FastSlowKernel
from proto.scheduler.scheduler import Scheduler
from diag.compute.metrics import compute_d1_offline_non_degenerate, compute_d3_port_loop

def run_experiment(config_path):
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
        
    # Setup
    torch.manual_seed(config['seed'])
    np.random.seed(config['seed'])
    
    # Init Env
    env = PointMassEnv(dt=0.1)
    obs = env.reset(seed=config['seed'])
    
    # Init Kernel
    dim_q = config['dim_q']
    k_type = config['kernel_type']
    if k_type == 'symplectic':
        kernel = SymplecticKernel(dim_q)
    elif k_type == 'contact':
        kernel = ContactKernel(dim_q, damping=config['damping'])
    elif k_type == 'fast_slow':
        kernel = FastSlowKernel(dim_q, damping=config['damping'], epsilon=config['epsilon'])
    else:
        raise ValueError(f"Unknown kernel type: {k_type}")
        
    x_int = kernel.init_state(batch_size=1)
    
    # Init Scheduler
    scheduler = Scheduler(config)
    
    # Logs
    log = {
        "x_int": [],
        "u_env": [],
        "u_self": [],
        "phase": [],
        "x_ext": []
    }
    
    # Loop
    total_steps = config['total_steps']
    u_self = torch.zeros(1, dim_q) # Initial write-back
    
    for t in range(total_steps):
        # 1. Scheduler
        sched_info = scheduler.step()
        phase = sched_info['phase']
        
        # 2. Env Step (Get x_ext)
        # For PointMass, we can just observe current state. 
        # But we need to step the environment to have dynamic x_ext.
        # Let's assume Env evolves autonomously or driven by random policy/input if "online".
        # For verifying "Experience Conditioned", env should have pattern.
        # Here we just step env with random action or zero to simulate "World moves".
        env_action = np.random.uniform(-0.1, 0.1, size=2) # Random world drift
        obs, _, _, _ = env.step(env_action)
        x_ext = torch.tensor(obs['x_ext_proxy'], dtype=torch.float32).unsqueeze(0) # [1, 4]
        
        # 3. Input Processing
        # u_env from x_ext? 
        # Let's map x_ext (4dim) to u_env (2dim) via simple linear projection or just taking pos.
        u_env = x_ext[:, :dim_q] 
        
        # u_self comes from previous step's write-back (not implemented fully in Kernel yet, so assuming Identity/Policy)
        # Let's assume u_self is generated by a "Policy" from x_int.
        # Simplified: u_self = -0.1 * q (stabilizing force idea) or just 0 for baseline.
        # But D3 needs u_self. Let's make u_self = 0.5 * p (momentum drive).
        if hasattr(kernel, 'dim_q'):
             # x_int has q, p, maybe s
             p_curr = x_int[:, dim_q:2*dim_q]
             u_self = 0.5 * p_curr
        
        u_t = scheduler.process_input(u_env, u_self, sched_info)
        
        # 4. Kernel Step
        x_int_next = kernel(x_int, u_t)
        
        # 5. Logging
        log["x_int"].append(x_int.detach().numpy()) # [1, Dim]
        log["u_env"].append(u_env.detach().numpy())
        log["u_self"].append(u_self.detach().numpy())
        log["phase"].append(phase)
        log["x_ext"].append(x_ext.detach().numpy())
        
        x_int = x_int_next
        
    # Process Logs
    traj_x_int = torch.tensor(np.array(log["x_int"])).squeeze(1) # T x Dim
    traj_u_self = torch.tensor(np.array(log["u_self"])).squeeze(1)
    
    # Extract Offline Segment
    phases = np.array(log["phase"])
    offline_mask = phases == "offline"
    
    # If no offline, warn
    if not np.any(offline_mask):
        print("Warning: No offline phase found.")
        d1 = {}
    else:
        # Get q, p from x_int. 
        # dim_q is 2. x_int: [q1, q2, p1, p2, (s)]
        q_off = traj_x_int[offline_mask, :dim_q]
        p_off = traj_x_int[offline_mask, dim_q:2*dim_q]
        # p is velocity-like
        d1 = compute_d1_offline_non_degenerate(q_off.unsqueeze(1), p_off.unsqueeze(1))
    
    # D3: Whole trajectory or just port loop active?
    d3 = compute_d3_port_loop(traj_x_int[:, :dim_q].unsqueeze(1), traj_u_self.unsqueeze(1))
    
    # Report
    report_dir = config['output_dir']
    run_name = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(os.path.join(report_dir, run_name), exist_ok=True)
    
    report_path = os.path.join(report_dir, run_name, "report.md")
    with open(report_path, 'w') as f:
        f.write(f"# Experiment Report: {run_name}\n")
        f.write(f"Config: {config}\n\n")
        f.write("## D1: Offline Non-degenerate\n")
        for k, v in d1.items():
            f.write(f"- {k}: {v:.4f}\n")
        f.write("\n## D3: Port Loop Amplification\n")
        for k, v in d3.items():
            f.write(f"- {k}: {v:.4f}\n")
            
    print(f"Run completed. Report saved to {report_path}")
    print("D1 Metrics:", d1)
    print("D3 Metrics:", d3)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True)
    args = parser.parse_args()
    
    run_experiment(args.config)
