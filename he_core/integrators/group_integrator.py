import torch
import torch.nn as nn
import math
from typing import Optional, Tuple, Callable
from torch.utils.checkpoint import checkpoint

class GroupContactState:
    """
    State for Group Contact Integrator on SO(1, n).
    """
    def __init__(self, G: torch.Tensor, M: torch.Tensor, z: torch.Tensor):
        self.G = G  # [B, D+1, D+1] in SO(1, n)
        self.M = M  # [B, D+1, D+1] in so(1, n) (Lie Algebra)
        self.z = z  # [B, 1] Contact variable (Action/Entropy)

    @property
    def batch_size(self) -> int:
        return self.G.shape[0]

    @property
    def dim(self) -> int:
        return self.G.shape[-1] - 1

    def clone(self):
        return GroupContactState(self.G.clone(), self.M.clone(), self.z.clone())

class GroupContactIntegrator(nn.Module):
    """
    Symplectic/Geometric Integrator for Contact Dynamics on SO(1, n).
    
    Dynamics:
    1. Geodesic flow on SO(1, n) generated by M.
    2. Dissipation and forcing applied in the algebra.
    3. Contact variable z evolves according to thermodynamics.
    
    References:
    - "Geometric Numerical Integration", Hairer et al.
    - "Lie-group methods", Iserles et al.
    """
    def __init__(self, dim: int, damping: float = 0.1):
        super().__init__()
        self.dim = dim
        self.damping = damping
        # Numerical safety: limit the per-step boost increment (rapidity) to avoid
        # cosh/sinh overflow in float32. This does NOT clip the state; it triggers
        # sub-stepping so that total dt is preserved.
        self.max_step_rapidity = 50.0
        # Absolute safety bound: if dynamics require too many sub-steps, the
        # computational graph can explode and trigger OOM. We fail-fast with a
        # clear error instead of letting the process die with CUDA OOM.
        self.max_substeps = 4096
        # When n_steps is large, backprop through every micro-step can OOM.
        # Use gradient checkpointing in chunks to trade compute for memory.
        self.checkpoint_chunk = 32
        
        # Minkowski Metric J = diag(-1, 1, ..., 1)
        self.register_buffer('J', torch.eye(dim + 1))
        self.J[0, 0] = -1.0

    def _minkowski_inner(self, u: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
        """Compute <u, v>_J = u^T J v"""
        # u, v: [B, D+1]
        # J: [D+1, D+1]
        return (u @ self.J * v).sum(dim=-1)

    def _renormalize(self, G: torch.Tensor) -> torch.Tensor:
        """
        Hyperbolic Gram-Schmidt Orthogonalization to project G back to SO(1, n).
        """
        # G: [B, D+1, D+1]
        B, N, _ = G.shape
        G_new = G.clone()
        
        # 1. Normalize first column (time-like)
        v0 = G_new[:, :, 0] # [B, N]
        norm_sq = self._minkowski_inner(v0, v0) # [B]
        # norm_sq should be -1. Scale = 1/sqrt(|norm_sq|)
        scale0 = 1.0 / torch.sqrt(torch.abs(norm_sq) + 1e-8)
        G_new[:, :, 0] *= scale0.unsqueeze(1)
        
        basis = [G_new[:, :, 0]]
        
        # 2. Orthogonalize rest (space-like)
        for k in range(1, N):
            vk = G_new[:, :, k]
            
            # Subtract projections onto previous basis vectors
            for j in range(k):
                ej = basis[j]
                # <vk, ej>_J / <ej, ej>_J
                # <ej, ej>_J is -1 for j=0, +1 for j>0
                inner = self._minkowski_inner(vk, ej)
                denom = -1.0 if j == 0 else 1.0
                proj = inner / denom
                vk = vk - proj.unsqueeze(1) * ej
            
            # Normalize
            norm_sq = self._minkowski_inner(vk, vk)
            # norm_sq should be +1
            scale = 1.0 / torch.sqrt(torch.abs(norm_sq) + 1e-8)
            vk = vk * scale.unsqueeze(1)
            
            G_new[:, :, k] = vk
            basis.append(vk)
            
        return G_new

    def _boost_from_vec(self, vec: torch.Tensor) -> torch.Tensor:
        """Compute exp(M(vec)) for a pure boost generator vec (analytic, no clipping).

        vec corresponds to the boost parameter; the rapidity is r=||vec||.
        Numerical stability is handled by small-r Taylor and (outside) sub-stepping.
        """
        # vec: [B, D]
        B_size, D = vec.shape
        dim = D + 1
        device = vec.device

        r = torch.norm(vec, dim=1, keepdim=True) + 1e-8
        
        # Coefficients
        # A = sinh(r)/r
        # B = (cosh(r)-1)/r^2
        
        # Numerical Stability for small r
        # If r < 1e-4, use Taylor expansion
        # sinh(r)/r = 1 + r^2/6 + ...
        # (cosh(r)-1)/r^2 = 1/2 + r^2/24 + ...
        
        mask_small = r < 1e-4
        
        # Compute safe values
        sinh_r = torch.sinh(r)
        cosh_r = torch.cosh(r)
        
        c1_safe = sinh_r / r
        c2_safe = (cosh_r - 1.0) / (r**2)
        
        # Taylor approx
        r2 = r**2
        c1_taylor = 1.0 + r2 / 6.0
        c2_taylor = 0.5 + r2 / 24.0
        
        coeff_1 = torch.where(mask_small, c1_taylor, c1_safe)
        coeff_2 = torch.where(mask_small, c2_taylor, c2_safe)
        
        res = torch.eye(dim, device=device).unsqueeze(0).repeat(B_size, 1, 1)
        # (0,0)
        res[:, 0, 0] = cosh_r.squeeze()
        # (0,1:) and (1:,0)
        v_scaled = vec * coeff_1
        res[:, 0, 1:] = v_scaled
        res[:, 1:, 0] = v_scaled
        # (1:,1:)
        vvT = vec.unsqueeze(2) * vec.unsqueeze(1)
        res[:, 1:, 1:] += coeff_2.unsqueeze(2) * vvT
        return res

    def _right_multiply_boost(self, G: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:
        """Compute G @ exp(M(vec)) for a pure boost vec without materializing exp(M).

        exp(M(vec)) has block form:
            [[a, b^T],
             [b, I + c vv^T]]
        where r=||v||, a=cosh r, b=v*sinh(r)/r, c=(cosh r-1)/r^2.

        This avoids allocating a full (D+1)x(D+1) matrix per micro-step and
        reduces overhead in tight loops.
        """
        # G: [B, D+1, D+1], vec: [B, D]
        B, N, _ = G.shape
        D = N - 1

        g0 = G[:, :, 0]          # [B, D+1]
        Gs = G[:, :, 1:]         # [B, D+1, D]

        r = torch.norm(vec, dim=1, keepdim=True) + 1e-8  # [B,1]
        r2 = r * r

        mask_small = r < 1e-4

        sinh_r = torch.sinh(r)
        cosh_r = torch.cosh(r)

        # b = vec * (sinh(r)/r)
        c1_safe = sinh_r / r
        c2_safe = (cosh_r - 1.0) / r2

        c1_taylor = 1.0 + r2 / 6.0
        c2_taylor = 0.5 + r2 / 24.0

        c1 = torch.where(mask_small, c1_taylor, c1_safe)  # [B,1]
        c2 = torch.where(mask_small, c2_taylor, c2_safe)  # [B,1]

        a = cosh_r  # [B,1]
        b = vec * c1  # [B,D]

        # new first column: a*g0 + Gs@b
        Gs_b = torch.einsum('bid,bd->bi', Gs, b)  # [B,D+1]
        new_g0 = a * g0 + Gs_b

        # shared term: (Gs@vec) for the rank-1 update in spatial block
        Gs_v = torch.einsum('bid,bd->bi', Gs, vec)  # [B,D+1]
        rank = (c2 * Gs_v)  # [B,D+1]

        # new spatial columns: Gs + g0*b^T + rank*vec^T
        new_Gs = Gs + g0.unsqueeze(2) * b.unsqueeze(1) + rank.unsqueeze(2) * vec.unsqueeze(1)

        return torch.cat([new_g0.unsqueeze(2), new_Gs], dim=2)

    def _exp_boost(self, M: torch.Tensor) -> torch.Tensor:
        """
        Compute exp(M) efficiently assuming M is a pure boost.
        M = [[0, p^T], [p, 0]]
        """
        p = M[:, 1:, 0]  # [B, D]
        return self._boost_from_vec(p)

    def _make_boost_matrix(self, vec: torch.Tensor) -> torch.Tensor:
        """
        Construct G = exp(M(vec)) where M(vec) is the boost generator for vector vec.
        This is equivalent to _exp_boost but takes the vector directly.
        """
        return self._boost_from_vec(vec)

    def step(self, state: GroupContactState, force_algebra: torch.Tensor, dt: float,
             H_val: Optional[torch.Tensor] = None,
             dH_ds: Optional[torch.Tensor] = None,
             pdH_dp: Optional[torch.Tensor] = None,
             v_override: Optional[torch.Tensor] = None,
             assume_safe_dt: bool = False) -> GroupContactState:
        """
        Perform one integration step.
        
        Args:
            state: Current state (G, M, z)
            force_algebra: Forcing term in so(1, n) [B, D+1, D+1]
            dt: Time step
            H_val: Value of Hamiltonian at current state [B, 1] (Optional)
            dH_ds: Derivative of H w.r.t s [B, 1] (Optional)
        """
        G, M, z = state.G, state.M, state.z
        
        # 1. Update Momentum (Lie Algebra)
        # M_dot = [M, Force] - damping * M
        # Strictly: M_dot = Force - p * (dH/ds)
        # Force is -dH/dq lifted to algebra.
        
        # Project force to so(1, n) just in case
        force_proj = 0.5 * (force_algebra - self.J @ force_algebra.transpose(1, 2) @ self.J)

        # Velocity for configuration update.
        # Preferred: use the true contact kinematics dot_q = dH/dp provided by the caller.
        # Fallback: approximate dot_q from p via a scalar metric_inv estimate.
        p0 = M[:, 0, 1:]  # [B, D]
        p0_norm = torch.norm(p0, dim=1)  # [B]
        if v_override is not None:
            metric_inv_est = None
            v0_norm = torch.norm(v_override, dim=1)
        elif pdH_dp is not None:
            p0_norm_sq = (p0 * p0).sum(dim=1, keepdim=True)
            metric_inv_est = pdH_dp / (p0_norm_sq + 1e-8)  # [B,1]
            v0 = metric_inv_est * p0
            v0_norm = torch.norm(v0, dim=1)
        else:
            metric_inv_est = None
            v0_norm = p0_norm

        # Sub-stepping inside the group integrator is only meant as a last-resort safety.
        # For correct adaptive stepping (recomputing forces per micro-step), do it in
        # ContactIntegrator and call this with assume_safe_dt=True.
        if assume_safe_dt:
            n_steps = 1
            dt_sub = dt
        else:
            max_inc = (dt * v0_norm).max().item() if v0_norm.numel() > 0 else 0.0
            if not math.isfinite(max_inc):
                raise RuntimeError(
                    "GroupContactIntegrator: non-finite rapidity increment detected "
                    f"(max_inc={max_inc}). This indicates the state already contains NaN/Inf "
                    "or momentum blew up beyond representable range."
                )
            n_steps = max(1, int(math.ceil(max_inc / float(self.max_step_rapidity))))
            if n_steps > int(self.max_substeps):
                raise RuntimeError(
                    "GroupContactIntegrator: adaptive sub-stepping exploded "
                    f"(n_steps={n_steps} > max_substeps={self.max_substeps}). "
                    f"This typically happens when ||p|| grows too large. "
                    f"dt={dt}, max_inc={max_inc:.3g}, max||p||={float(p0_norm.max().item()):.3g}. "
                    "Fix the underlying instability (or reduce dt) instead of letting it OOM."
                )
            dt_sub = dt / n_steps
        
        # Damping/friction coefficient a = dH/ds (Contact term).
        # Note: we do NOT clamp a here (that would change dynamics). Numerical stability
        # is achieved via an integrating-factor update which is exact for dp/dt = F - a p.
        if dH_ds is not None:
            a = dH_ds  # [B, 1]
        else:
            a = torch.full((G.shape[0], 1), float(self.damping), device=G.device, dtype=G.dtype)
            
        # Sub-stepped integration (no clipping): repeat implicit update + Lie group update.
        G_new = G
        M_new = M
        z_new = z

        a_dt = a * dt_sub
        exp_term = torch.exp(-a_dt)
        a_abs = torch.abs(a)
        phi = torch.where(a_abs < 1e-6, torch.full_like(a, dt_sub), (1.0 - exp_term) / a)
        F_vec_const = force_proj[:, 0, 1:]

        if pdH_dp is not None and H_val is not None:
            H_safe = torch.nan_to_num(H_val)
            dot_z_const = pdH_dp - H_safe
        elif H_val is not None:
            dot_z_const = -torch.nan_to_num(H_val)
        else:
            dot_z_const = None

        def microstep(G_step: torch.Tensor, p_step: torch.Tensor, z_step: torch.Tensor, steps: int):
            G_local = G_step
            p_local = p_step
            z_local = z_step
            for _ in range(int(steps)):
                p_local = exp_term * p_local + phi * F_vec_const

                if v_override is not None:
                    v_local = v_override
                elif metric_inv_est is not None:
                    v_local = metric_inv_est * p_local
                else:
                    v_local = p_local
                G_local = self._right_multiply_boost(G_local, dt_sub * v_local)

                if dot_z_const is not None:
                    z_local = z_local + dt_sub * dot_z_const
                else:
                    z_local = z_local + dt_sub * (-float(self.damping) * z_local)
            return G_local, p_local, z_local

        p_work = M_new[:, 0, 1:]

        # Chunked stepping with checkpoint to control memory when n_steps is large.
        chunk = int(self.checkpoint_chunk)
        use_ckpt = (
            torch.is_grad_enabled()
            and (G_new.requires_grad or p_work.requires_grad or z_new.requires_grad)
            and n_steps > chunk
        )

        remaining = int(n_steps)
        while remaining > 0:
            step_now = min(chunk, remaining)
            if use_ckpt:
                G_new, p_work, z_new = checkpoint(
                    lambda g, p, z: microstep(g, p, z, step_now),
                    G_new,
                    p_work,
                    z_new,
                    use_reentrant=False,
                )
            else:
                G_new, p_work, z_new = microstep(G_new, p_work, z_new, step_now)
            remaining -= step_now

        M_new = torch.zeros_like(M_new)
        M_new[:, 0, 1:] = p_work
        M_new[:, 1:, 0] = p_work
        
        return GroupContactState(G_new, M_new, z_new)

    def flat_to_group(self, q: torch.Tensor, p: torch.Tensor) -> GroupContactState:
        """
        Map flat (q, p) to Group State (G, M).
        q in R^D -> Hyperboloid -> G in SO(1, D)
        p in R^D -> Algebra -> M in so(1, D)
        """
        B, D = q.shape
        device = q.device
        
        # 1. Map q to Group Element G
        # Instead of Gram-Schmidt, we construct the pure boost matrix directly.
        # This is O(1) instead of O(N^2).
        G = self._make_boost_matrix(q)
        
        # 2. Map p to Algebra M
        # p is tangent vector at q.
        # Lift to algebra: M = x ^ p (wedge product) roughly
        # In so(1, n), M = [[0, u^T], [u, 0]] for boosts
        # We map p to the boost part.
        
        M = torch.zeros(B, D+1, D+1, device=device)
        # p corresponds to velocity in flat space.
        # Map to velocity in hyperbolic space?
        # For small q, p ~ velocity.
        # Let's just map p to the boost components (row 0, col 1..D)
        
        M[:, 0, 1:] = p
        M[:, 1:, 0] = p
        
        z = torch.zeros(B, 1, device=device)
        
        return GroupContactState(G, M, z)

    def group_to_flat(self, state: GroupContactState) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Map Group State (G, M) back to flat (q, p).
        """
        G, M = state.G, state.M
        
        # 1. Extract q from G
        # q is derived from the first column of G (position on H^D)
        x = G[:, :, 0] # [B, D+1]
        # x = (x0, x1...xD)
        # Invert x = (cosh r, sinh r * dir)
        # r = acosh(x0)
        x0 = x[:, 0].clamp(min=1.0 + 1e-7)
        r = torch.acosh(x0).unsqueeze(1)
        
        spatial = x[:, 1:]
        spatial_norm = torch.norm(spatial, dim=1, keepdim=True) + 1e-8
        dir_q = spatial / spatial_norm
        
        q = r * dir_q
        
        # 2. Extract p from M
        # Take the boost components
        p = M[:, 0, 1:]
        
        return q, p
