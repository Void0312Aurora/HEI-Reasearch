DEBUG: Module Loading
DEBUG: Script Entry
=== Phase 20: MNIST Adaptive Damping Training ===
Mode: adaptive, Drive: 1.0
Loading RawMNIST from ./data/MNIST/raw/train-images-idx3-ubyte.gz...
Loaded 60000 samples.
Loading all data into memory...
Data Loaded: torch.Size([60000, 1, 28, 28])
Epoch 0 [0/1875]: Loss=2.3078 (Cls=2.3078, Diss=0.0055)
CRASH: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
Traceback (most recent call last):
  File "/home/void0312/HEI-Research/HEI/EXP/train_mnist_adaptive.py", line 272, in <module>
    train_mnist()
  File "/home/void0312/HEI-Research/HEI/EXP/train_mnist_adaptive.py", line 241, in train_mnist
    final_loss.backward()
  File "/home/void0312/miniconda3/envs/PINNs/lib/python3.11/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/void0312/miniconda3/envs/PINNs/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/void0312/miniconda3/envs/PINNs/lib/python3.11/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
DEBUG: Script Exit
