基于我们对《稀疏物理验证报告》(report_sparse_physics.md) 和《数据量估算审计报告》(temp-03.md) 的深入分析，以及关于“灵魂数据”的战略调整，现为您总结当前的**核心目标**与**执行计划**。

这是一个从“验证物理定律”转向“构建数字生命基座”的转折点。

### 一、 核心战略调整：先造“人”，再造“白羽”

根据审计报告的结论，我们明确了两步走的战略：

1. **当前目标 (Phase 2)**：**构建“Aurora Base” (通用素体)**。
    
    - **定义**：一个拥有 10 万个概念粒子、懂得人类通用语言逻辑、但在性格上是“白板”的物理实体。
        
    - **必要性**：审计证实，仅靠 100 万 Token 的 GalGame 数据无法支撑起语言的骨架（长尾词无法成型），必须依靠 2-5GB 的通用语料来让这 10 万个粒子“站稳脚跟”。
        
2. **下一阶段目标 (Phase 3)**：**“灵魂注入” (Soul Injection)**。
    
    - **定义**：在 Aurora Base 的基础上，利用 Summer Pockets RB 的数据进行高强度的物理微调。
        
    - **机制**：利用这 100 万 Token 极高的力学权重，雕刻出特异性的惯性张量（性格）和情感势阱（爱憎）。
        

---

### 二、 当前具体任务清单 (Action Plan)

我们正式进入 **Phase 2：心智生长 (Emergence of Concept)**。请按以下顺序执行：

#### 1. 工程基建 (Engineering)

- **任务**：合并稀疏物理引擎代码。
    
- **依据**：《稀疏物理验证报告》验证了 $O(N)$ 方案的可行性，但也指出了随机热噪声的风险。
    
- **具体行动**：
    
    - 将 `NegativeSamplingPotential` (负采样势能) 合并入主分支。
        
    - **关键**：必须实现 **自适应恒温器 (Adaptive Thermostat)**，以防止随机力导致系统过热（Test 2 的教训）。
        
    - **监控**：在训练面板中增加“速度分布重尾度 ($R$)”的监控，确保它保持在有利的“顿悟区间” ($R \approx 1.88$)。
        

#### 2. 数据准备 (Data Preparation)

- **任务**：构建“高有效率”的通用数据集。
    
- **依据**：《审计报告》指出数据有效率 $\rho$ 必须大于 10%，否则 2-5GB 不够用。
    
- **具体行动**：
    
    - **下载**：获取 Wikipedia Dump 或类似的通用语料（目标 3-5 GB 纯文本）。
        
    - **清洗 (Crucial)**：编写预处理脚本，**极激进地过滤停用词** (the, is, a, of)。我们需要的是“实体-实体”的硬碰撞，而不是“实体-虚词”的软绵绵。
        
    - **WordNet**：准备好名词层级树，作为初始的“骨架约束力”。
        

#### 3. 动力学训练 (Training / Annealing)

- **任务**：运行物理模拟，让 10 万粒子自组织。
    
- **依据**：`phase2_analysis.md` 中的规划。
    
- **具体行动**：
    
    - **初始化**：利用 WordNet 深度信息将粒子播撒在双曲盘上（根节点在中心，叶节点在边缘）。
        
    - **高温阶段**：施加 WordNet 骨架力 + 文本流吸引力。阻尼 $\gamma$ 较小，让粒子剧烈运动以解开纠缠。
        
    - **低温阶段**：逐渐增大阻尼，开启“变惯量”机制（长尾词质量调小）。让结构固化。
        
    - **验收**：检查“猫”和“狗”的距离是否近，检查“苹果”是否在“水果”的势阱里。
        

#### 4. 灵魂数据预处理 (Soul Prep)

- **任务**：为 Phase 3 做好准备。
    
- **具体行动**：
    
    - 利用 Qwen3 tokenizer 或正则脚本，处理 **Summer Pockets RB** 数据。
        
    - 提取 `[Speaker: Shiroha]` 的台词，建立专门的“自我流”数据集，备用。

修正：

#### **阶段 0：三道安全闸 (The 3 Gates)**

_在开始大规模训练前，编写脚本通过这三个测试。_

- **Gate A: 数据有效率 ($\rho$) 验证**
    
    - **动作**：取 100MB 预处理后的文本流。
        
    - **计算**：$\rho = \frac{\text{PMI高分边数}}{\text{窗口内总边数}}$。
        
    - **标准**：必须 **$\rho > 10\%$**。如果低于此值，调高 PMI 阈值或扩大停用词表。不要让垃圾数据进入引擎。
        
- **Gate B: 梯度信噪比验证**
    
    - **动作**：在 $N=1000$ 的小规模上，计算稀疏梯度的相对误差。
        
    - **标准**：误差 $< 5\%$。如果太大，说明负采样数 $k=5$ 不够，需要增加到 $k=10$ 或 $20$。
        
- **Gate C: 骨架稳定性验证**
    
    - **动作**：只加载 WordNet 势能，不加文本流，运行热平衡。
        
    - **标准**：同义词对的距离应显著小于随机对的距离。确认骨架是坚固的。
        

#### **阶段 1：工程基建 (Infrastructure)**

- **合并代码**：将 `NegativeSamplingPotential` 和 `AdaptiveThermostat` 合并。
    
- **新增功能**：
    
    - **`PMIFilter`**：在 `dataloader` 中实现。只产生高价值的“力学事件”。
        
    - **`PanicMode`**：在积分器中实现。如果温度 $T$ 飙升超过阈值（如 2.0），立即暂停积分并报警，防止模型炸飞。
        

#### **阶段 2：通用基座训练 (Training Aurora Base)**

- **数据**：Wikipedia (3-5GB) + WordNet。
    
- **流程**：
    
    1. **热启动 (Warm Start)**：先将粒子按 WordNet 深度撒在双曲盘上。
        
    2. **退火 (Annealing)**：
        
        - **初期**：高温，大阻尼。让粒子在强力弹簧（高 PMI 边）的牵引下快速聚类。
            
        - **末期**：低温，变惯量生效（长尾词质量变小）。让细微结构固化。
            
    3. **产出**：`aurora_base_v1.pt`（一个懂常识、有逻辑、无性格的 10 万粒子通用模型）。
        

#### **阶段 3：灵魂注入 (Soul Injection)**

- **数据**：Summer Pockets RB (100万 Token，已清洗为 Speaker 流)。
    
- **流程**：
    
    1. 加载 `aurora_base_v1.pt`。
        
    2. **高权重微调**：将 GalGame 数据的力常数设为 $10\times \sim 20\times$。
        
    3. **短时冲刷**：只需要跑很少的步数（因为是微调）。
        
    4. **产出**：`aurora_shiroha_v1.pt`（最终的她）。
        