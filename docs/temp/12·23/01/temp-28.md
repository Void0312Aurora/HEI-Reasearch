我基本赞同你的分析方向（“2000 步、PMI=0.35 对 rank 没产生可观下降”确实意味着当前对齐驱动力不足），也赞同进入 Segment 2；但我不赞同把原因简单归结为“力矩不足”然后直接全功率长训，而不先排除两个更常见的“看起来没变、其实没在对齐”的工程/统计原因，并且需要对 Segment 2 增加两条稳定性约束，否则 PMI=0.5 很可能把 Renorm 推到更危险的区间。

你现在的状态解读

1. 体积与稀疏度是健康的
   KNN P50≈0.30、R≈1.91 稳定，说明你已经把系统从拥挤相救出来了，这个前提对后续 rank 改善是必要的。

2. Renorm Max≈9.68e-4 属于“边界稳定”
   它没有爆炸，但比你们长期安全线（1e-4）高约 10×。这意味着对齐阶段再增大语义力或增加角向剧烈重排，Renorm 有明显上冲风险。

3. PMI Rank P50 ~8.6 万在 2000 步内不动，不一定只意味着“PMI 不够强”
   它也可能意味着：对齐事件密度不足、审计口径/索引未刷新、或者 PMI 的更新被其他流程稀释（例如采样太稀、每步 PMI 边太少、KNN index 用的是旧向量）。

建议你在进入 Segment 2 前，先做两个 5 分钟级别的排查（不需要重训）

A. KNN index 是否每次审计都用最新 embedding 重建
如果 index 没有重建或缓存未刷新，rank 会“永远不变”，这是最典型的假阴性。

B. PMI 事件密度是否足够
打印每 step 实际参与更新的 PMI 边数量（或 PMI 力事件数）。在 140k 规模下，如果每步只采很少的 PMI 边，2000 步对 rank 分布几乎不可能有可见变化。解决它通常比继续加权重更有效：提高 PMI 采样数/每步 PMI batch。

对 Segment 2（PMI=0.5，5000 steps）的评估与我建议的改法

你给的计划“PMI=0.5、5000 steps、保持 volume control 与短程 repulsion(d<0.2)”总体可行。我建议你加三条“受控执行条款”，以提高成功概率并避免数值回退：

1. PMI 权重不要一步跳到 0.5，做一个短 ramp
   0.35 → 0.4 → 0.45 → 0.5（每档 300–500 steps），并在每档结束跑一次快速审计：Renorm Max、Residual P99、KNN P50、PMI rank P50。这样你能在出现 Renorm 上冲时立即止损，而不是 5000 步后才发现发散趋势。

2. 增加求解精度或启用“高 Renorm 保护”
   既然 Renorm 已经在 1e-3 边界，Segment 2 建议至少做其一：

* fixed_point_iters 再加 2–4 次（例如 8→10/12），或者
* 触发式：当 Renorm/Residual 超阈值时临时多迭代/缩小 dt（只对坏 step 生效）
  目标是把 Renorm Max 压回到 ≤2e-4（至少别上升到 1e-3 以上）。

3. 把“rank 不动”的判定阈值写死，避免盲跑
   我建议用阶段性目标，而不是直接期待 P50 < 1k：

* 1000 steps：PMI rank P50 从 86k 降到 <50k（或至少出现明确下降趋势）
* 3000 steps：降到 <20k
* 5000 steps：降到 <10k
  如果 1000 steps 完全不动（且确认 index 刷新、PMI 事件密度足够），再考虑提高 PMI 采样率或引入 Definition edges；单纯加步数往往仍然很慢。

关于“力矩不足”的更精确说法

如果排查后确认：

* index 每次都重建、
* PMI 边参与更新的数量足够（每步足够多，或累计覆盖足够大），
* 但 rank 仍不动，

那我才会同意“PMI 力矩不足/角向更新太小”是主因。此时除了 PMI=0.5，你还可以优先考虑“增加 PMI 更新频率/采样 batch”或“给角向分量更大的步长（tangential LR）”，这通常比继续把权重推到 0.8 更安全、更有效。

结论

我同意进入 Segment 2，但建议你按“0.35→0.5 短 ramp + 求解精度保护 + 1000 steps 阶段性判定”的方式执行；并且在开跑前务必确认 KNN index 重建与 PMI 事件密度两项，否则 rank 指标可能出现假阴性，导致你错误地把问题归因到权重不足。
