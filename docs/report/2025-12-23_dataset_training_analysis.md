# 数据集训练实现缺陷分析报告 (更新版)

**日期：** 2025-12-23
**对象：** HEI 数据集训练流程 (`scripts/train_aurora_base_gpu.py`, `datasets`, `tools`)

## 执行摘要
经过深度调研，确认项目包含真实的 wiki 数据和 PMI 计算脚本 (`build_wiki_pmi.py`)，这解决了之前关于“缺乏真实数据”的担忧。然而，**核心的架构缺陷（索引耦合）依然存在**，并且在真实的大规模数据处理脚本中更为致命。目前的流程是一个脆弱的闭环：`训练基础模型 -> 生成 Checkpoint -> 提取词表 -> 计算 PMI 索引 -> 再次训练`。任何环节的变动（如词表截断）都会导致索引错位，进而破坏训练。

## 严重缺陷

### 1. 语义边中的索引耦合（脆弱性）
**严重程度：** 严重 (Critical)
**位置：** `scripts/tools/build_wiki_pmi.py` (及 `semantic_loader.py`)

-   **问题：** `build_wiki_pmi.py` 这是一个完善的 MapReduce 脚本，它加载一个**特定 Checkpoint**，提取其词表和索引，计算 PMI，然后将边保存为 `(id_u, id_v, weight)` 的**整数索引**形式。
-   **后果：** 这个生成的 `semantic_edges_wiki.pkl` 文件被永久绑定到了生成它的那个特定 Checkpoint 上。
    -   如果我们在训练时改变了 `--limit`（例如从 全量 改为 10000），词表的 ID 映射会改变，导致加载的 PMI 边指向错误的单词。
    -   如果我们重新生成了基础 Checkpoint（例如改变了随机种子或初始化逻辑），ID 也可能漂移。
-   **结果：** 这是一个**隐蔽的数据破坏炸弹**。如果在不匹配的 Checkpoint 上加载 PMI 边，模型将受到错误的语义力拉扯，且不会报错。

### 2. 循环依赖的工作流（工程缺陷）
**严重程度：** 中 (Medium)
**位置：** 工作流架构

-   **问题：** 为了生成 PMI 边，必须先有一个训练好的 Aurora Base Checkpoint（用于提供词表）。为了训练一个好的 Aurora 模型，又需要这些 PMI 边。
-   **现状：** 目前似乎采用“先训练一个空语义的基础版” -> “生成 PMI” -> “加载 PMI 再次训练语义版”的流程。
-   **建议：** 应该解耦词表生成。词表应该由数据集（Cilin/HowNet）直接定义，作为一个静态资产（`vocab.json`），而不是依赖于动态的 Checkpoint。

### 3. 先前误判更正：真实数据存在
-   **调研确认：** 存在 `data/wiki/wikipedia-zh-20250901.json` (2.2GB) 和 `build_wiki_pmi.py`。之前的“缺乏真实数据”结论是基于 `build_semantic_edges.py` (Mock 工具) 的误判。
-   **新问题：** 虽然数据存在，但加载器 (`semantic_loader.py`) 仍然存在忽略 `node_map` 的逻辑错误，这使得即使有真实数据，也无法安全加载。

## 改进建议

1.  **重构语义边存储方式（最优先）：**
    *   **方案：** 修改 `build_wiki_pmi.py`，使其保存 **字符串三元组** `("单词A", "单词B", PM值)` 而非整数索引。
    *   **收益：** 彻底解耦 PMI 数据与 Checkpoint。同一个 PMI 文件可以被任何版本的模型加载，只要模型包含这些词即可。加载时未找到的词可以直接忽略。
    
2.  **修复加载器逻辑：**
    *   修改 `src/hei_n/datasets/semantic_loader.py`，使其接收运行时的 `node_map`，并实时将字符串边转换为当前的索引。

3.  **静态化词表（可选，长期）：**
    *   建议生成一个 `vocab.json` 作为项目的“真理来源”，让 Checkpoint 和 PMI 工具都依赖这个静态文件，而不是互相依赖。

4.  **配置管理：**
    *   `build_wiki_pmi.py` 中的路径和参数（如窗口大小、阈值）应提取到配置文件或命令行参数中。
